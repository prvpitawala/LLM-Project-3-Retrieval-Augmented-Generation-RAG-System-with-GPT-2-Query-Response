{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Retrieval-Augmented Generation (RAG) System with GPT-2: Document Retrieval, Text Generation, and Query Response**\n",
        "\n",
        "\n",
        "This notebook demonstrates a Retrieval-Augmented Generation (RAG) system using GPT-2. It combines document retrieval and text generation by encoding documents, retrieving relevant ones based on queries, and generating coherent responses. It showcases a practical approach to enhancing query answering with GPT-2."
      ],
      "metadata": {
        "id": "z_nIAKfDFcOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **RAG (Retrieval-Augmented Generation**) system is a hybrid model that combines two powerful components: information retrieval and text generation. It is designed to answer questions or generate responses by retrieving relevant documents from a knowledge base and then generating a coherent answer based on that information."
      ],
      "metadata": {
        "id": "LnlOwOnQC3xT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "dXcZtCjbb1Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers -q\n",
        "!pip install transformers -q\n",
        "!pip install torch -q"
      ],
      "metadata": {
        "id": "pzQqcAhdbGH5"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "KlR4Ew8hb8uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "7tX9xfHkb_1K"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "936lJlHjhW3z",
        "outputId": "11943b7e-c9d5-4c90-8c71-0e150fdacb0c"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading pre-trained embedding model and GPT-2 model\n",
        "\n",
        "This is fine-tuned GPT-2 model specifically designed to answer questions based on the provided context. The model has been fine-tuned to generate accurate and contextually relevant responses"
      ],
      "metadata": {
        "id": "pV9V6JwNcFaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SentenceTransformer model for generating embeddings from text.\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Load the tokenizer for GPT-2 from the specified directory.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('/content/drive/MyDrive/gpt2-finetuned')\n",
        "\n",
        "# Load the fine-tuned GPT-2 model from the specified directory.\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/gpt2-finetuned/checkpoint-1680')"
      ],
      "metadata": {
        "id": "j2rbF5RccU7a"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seting the pad token to be the same as the eos token"
      ],
      "metadata": {
        "id": "a51KZqGQeTrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the padding token to be the same as the end-of-sequence (EOS) token.\n",
        "# This ensures that padding tokens used during text generation or batching\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "o1AoRW_UciXC"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example documents"
      ],
      "metadata": {
        "id": "frGN-ZkWeniy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"The Eiffel Tower is in Paris.\",\n",
        "    \"The Great Wall of China is in China.\",\n",
        "    \"Mount Everest is the highest mountain in the world.\",\n",
        "    \"Praveen is a machine learning engineer.\",\n",
        "    \"Mount Everest, the highest peak in the world.\",\n",
        "    \"The Mona Lisa is a famous painting by Leonardo da Vinci.\",\n",
        "    \"The Amazon Rainforest is located in South America.\",\n",
        "    \"The Pacific Ocean is the largest ocean on Earth.\",\n",
        "    \"Albert Einstein developed the theory of relativity.\",\n",
        "    \"The Pyramids of Giza are one of the Seven Wonders of the Ancient World.\",\n",
        "    \"The Sahara Desert is the largest hot desert in the world.\",\n",
        "    \"Vincent van Gogh was a Dutch post-impressionist painter.\",\n",
        "    \"The Taj Mahal is located in Agra, India.\",\n",
        "    \"The Grand Canyon is a large canyon in the state of Arizona, USA.\",\n",
        "    \"Shakespeare wrote many famous plays including 'Romeo and Juliet'.\",\n",
        "    \"The Colosseum is an ancient amphitheater located in Rome, Italy.\",\n",
        "    \"The Gal√°pagos Islands are known for their unique wildlife.\",\n",
        "    \"Leonardo da Vinci was also an inventor and scientist.\",\n",
        "    \"The Berlin Wall once divided East and West Berlin.\",\n",
        "    \"The Great Barrier Reef is the largest coral reef system in the world.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "iQyqAK04cllb"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating embeddings for documents"
      ],
      "metadata": {
        "id": "eYqIpcqScqd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the list of documents into vectors (embeddings) using the embedding model.\n",
        "doc_embeddings = embedding_model.encode(documents, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "FlMqBnIncrMY"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for retrieve document"
      ],
      "metadata": {
        "id": "5SER1KIAc0VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_documents(query, top_k=1):\n",
        "    # Encode the query into a vector (embedding) using the embedding model.\n",
        "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # Compute the cosine similarity between the query embedding and the document embeddings.\n",
        "    similarities = util.pytorch_cos_sim(query_embedding, doc_embeddings)\n",
        "\n",
        "    # Get the indices of the top_k most similar documents\n",
        "    top_results = torch.topk(similarities, k=top_k)\n",
        "\n",
        "    return [documents[idx] for idx in top_results.indices[0]]"
      ],
      "metadata": {
        "id": "J59VkiT7c7G3"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to generate a response using GPT-2"
      ],
      "metadata": {
        "id": "BVx_B2fQdACd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stop_generation(decoded_text):\n",
        "    # Check if '<que>' token is in the generated text\n",
        "    return '<que>' in decoded_text\n",
        "\n",
        "def generate_response(query):\n",
        "    # retrieve relevant document\n",
        "    relevant_docs = retrieve_documents(query)\n",
        "    context = \" \".join(relevant_docs)\n",
        "\n",
        "    # Prepare input for GPT-2\n",
        "    prompt = f\"{context}\\n<que>{query}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
        "\n",
        "    # Initialize generation process\n",
        "    generated_text = \"\"\n",
        "    input_ids = inputs.input_ids\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # Loop to generate text manually and stop if '<que>' token is detected\n",
        "    while True:\n",
        "        outputs = gpt2_model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=len(input_ids[0]) + 1,  # Generate 1 more tokens at a time\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            num_beams=5,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            early_stopping=False,  # Disable early stopping\n",
        "        )\n",
        "\n",
        "        # Decode the generated tokens and add to the current generated text\n",
        "        new_generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated_text += new_generated_text[len(generated_text):]  # Append new tokens\n",
        "\n",
        "        # Check if '<que>' token is found in the text\n",
        "        if stop_generation(generated_text):\n",
        "            break  # Stop if the '<que>' token is found\n",
        "\n",
        "        # Update input_ids for the next loop\n",
        "        input_ids = outputs\n",
        "\n",
        "    return generated_text.split(\"<que>\")[0].strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "WQRWSr5C-NoQ"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get predictions"
      ],
      "metadata": {
        "id": "8nsdd8fRDBYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the answers to a given question.\n",
        "query = \"who is praveen\"\n",
        "response = generate_response(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h79tyXybaO7",
        "outputId": "39dd20f5-7537-4d25-cfb2-9c923f7ba317"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Praveen is a machine learning engineer.\n"
          ]
        }
      ]
    }
  ]
}